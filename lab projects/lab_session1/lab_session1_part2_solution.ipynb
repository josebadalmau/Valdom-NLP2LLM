{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All Rights Reserved**\n",
    "\n",
    "**Copyright (c) 2025 IRT Saint-Exupery**\n",
    "\n",
    "*Author & contact:* \n",
    "* mouhcine.mendil@irt-saintexupery.com \n",
    "\n",
    "# Natural Language Processing (NLP) to Large Language Models (LLM)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h2>Lab Session 1: Part II</h2>\n",
    "</div>\n",
    "\n",
    "## Machine Translation\n",
    "\n",
    "Our task is to automatically translate sentences from English to French. We will not perform a literal word-to-word translation, as the solution is a straighforward word retrieval from a lookup table. Instead, we aim to train the translation model by showing it several examples of English sentences and French sentences.  \n",
    "\n",
    "Machine Translation is an exemple of sequence-to-sequence learning (Seq2Seq), which consists on training models to convert **sequences of variable length** from one domain (e.g. sentences in English) to **sequences of variable length** in another domain (e.g. the same sentences translated to French). This can be used when you need to generate text, such as in machine translation or text summarization. There are multiple ways to handle this task; **We will focus on RNNs** that you have learned previously.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "⚠️⚠️⚠️ Even if seq2seq models are suitable to handle variable-length sequences, they need to be trained on data with similar input sequence length $l_{\\text{input}}$ and output sequence length $l_{\\text{output}}$. We will see how to make this possible later in this notebook. ⚠️⚠️⚠️\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. English-French MT Dataset\n",
    "\n",
    "We want to train a model to learn English to French translation from a simple dataset hosted in http://www.manythings.org/anki/. Besides, the website provides many translations for other languages such as English, Spanish and Chinese. \n",
    "\n",
    "We have previously download the dataset, which you can find in `data/eng_to_fr.txt`.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 1 </b>\n",
    "\n",
    "- Read in a dataframe the first 20000 rows of the file <code>data/eng_to_fr.txt</code>. Make sure you are using the right separator.\n",
    "- Get a general sense of what the dataset is about and describe it. Is the length of the input and target sequences similar ?\n",
    "- Keep only the first two columns and name them <code>english</code> and <code>french</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# limit number of lines read\n",
    "df_mt = pd.read_csv(\n",
    "    \"data/eng_to_fr.txt\",\n",
    "    sep=\"\\t\",\n",
    "    nrows=20000,\n",
    "    header=None,\n",
    "    names=[\"english\", \"french\", \"comment\"],\n",
    ")\n",
    "df_mt = df_mt[[\"english\", \"french\"]]\n",
    "print(f\"Number of samples: {len(df_mt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Get up.</td>\n",
       "      <td>Lève-toi !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Get up.</td>\n",
       "      <td>Debout.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Va, maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Allez-y maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Go now.</td>\n",
       "      <td>Vas-y maintenant.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english               french\n",
       "0       Go.                 Va !\n",
       "1       Go.              Marche.\n",
       "2       Go.           En route !\n",
       "3       Go.              Bouge !\n",
       "4       Hi.              Salut !\n",
       "..      ...                  ...\n",
       "95  Get up.           Lève-toi !\n",
       "96  Get up.              Debout.\n",
       "97  Go now.      Va, maintenant.\n",
       "98  Go now.  Allez-y maintenant.\n",
       "99  Go now.    Vas-y maintenant.\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mt.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and preparation\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.1 </b>\n",
    "\n",
    "- To simplify the problem (smaller vocabulary), lower all capital letters.\n",
    "- Can we apply other data cleaning operations ? Explain your answer.\n",
    "- Split your data into training (80%) and test (20%) subsets using <code>train_test_split</code>, random seed = 42 and <code>shuffle</code> set to True.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "⚠️⚠️⚠️ Once done, note that vocabulary size and token index will be exclusively based on the train dataset. Make sure you choose the same random seed and other arguments specified in the question. ⚠️⚠️⚠️\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# lower case\n",
    "df_mt_clean = df_mt.map(lambda x: x.lower())\n",
    "\n",
    "# Split data into train and test\n",
    "df_mt_train, df_mt_test = train_test_split(\n",
    "    df_mt_clean, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "df_mt_train.reset_index(inplace=True, drop=True)\n",
    "df_mt_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Machine Translation, we will rely on sequence-to-sequence (Seq2Seq) models. \n",
    "\n",
    "### Seq2Seq: Encoder/Decoder\n",
    "\n",
    "Seq2Seq has been first introduced by [Google](https://arxiv.org/abs/1409.3215). It captures the information carried by the input sequences in a low-dimensional encoded form and generates relevant output sequences in an iterative manner. Seq2Seq relies on two main blocks: one to read the input sequence (encoder) and another to generate the output sequence (decoder). \n",
    "\n",
    "**Encoder**  \n",
    "The encoder processes the input sequence. It reads the input data **one token at a time** and transform it into a **context vector** (fixed-sized vector), capturing the data's essential information. The encoder (RNN, LSTM, or GRU) traverses the input sequence, updating its internal state at each step. By the end of this process, the internal state of the encoder is a compact representation of the entire input sequence.\n",
    "\n",
    "**Decoder**  \n",
    "The decoder is tasked with generating the output sequence. Starting from the **context vector** produced by the encoder, it generates the output elements one at a time. Like the encoder, the decoder is often implemented as an RNN, LSTM, or GRU. It uses the context vector and what it has generated so far to predict the next element in the output sequence. This process is iterative and continues until a special **end-of-sequence token** is generated (or some other stopping criterion).\n",
    "\n",
    "At each decoding step, the output of the decoder is passed through a **dense layer followed by a softmax function**. The softmax produces a probability distribution over the entire **target vocabulary**, allowing the model to select the most likely next token. This softmax layer is crucial, as it transforms the decoder’s output into a meaningful prediction at every timestep.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/seq2seqmodel.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Now we know the type of model we will use, let's move to tokenization. We will rely on character-to-character machine translation; therefore, sentences will be tokenized at <b>character-level</b>.\n",
    "\n",
    "Note however that there are two additional special tokens that we need for Seq2Seq models: the Start-of-Sequence (SOS) token and the End-of-Sequence (EOS). \n",
    "\n",
    "- The SOS is essential for initializing the decoding process. During training, it serves as the first input to the decoder, signaling the model to begin generating the target sequence. This helps the model learn a consistent starting context across all examples. During inference (i.e., when generating a translation), the SOS token is explicitly fed as the first decoder input, which triggers the model to begin producing the output sequence.\n",
    "\n",
    "- The End-of-Sequence (EOS) token, on the other hand, indicates when the generated sequence should stop. It is included in the target output during training so the model can learn to associate certain contexts with the end of a sentence. During inference, the model continues generating tokens until it predicts an EOS token, which signals that the output is complete. Without EOS, the model might continue generating irrelevant tokens or cut off the translation prematurely. Together, SOS and EOS provide clear boundaries for structured and meaningful sequence generation.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/tokenization.png\" width=\"40%\"/>\n",
    "  <figcaption>Author: Shann Khosla<figcaption/>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.2 </b>\n",
    "\n",
    "- Add a Start-Of-Sentence (SOS) characters `\\t` and End-Of-Sentence (EOS) character `\\n` to <b>each french sentence (target)</b>. For example, \"bonjour!\" becomes \"\\tbonjour!\\n\"\n",
    "- What is the maximum sequence length for input text (English) and target text (French) ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of characters per text for inputs (English): 17\n",
      "Max number of characters per text for outputs (French): 59\n"
     ]
    }
   ],
   "source": [
    "df_mt_train.french = df_mt_train.french.apply(lambda x: f\"\\t{x}\\n\")\n",
    "df_mt_test.french = df_mt_test.french.apply(lambda x: f\"\\t{x}\\n\")\n",
    "\n",
    "# Max seq lenght for english and french\n",
    "max_fr_seq_len = np.max(df_mt_train[\"french\"].map(len))\n",
    "max_en_seq_len = np.max(df_mt_train[\"english\"].map(len))\n",
    "\n",
    "print(f\"Max number of characters per text for inputs (English): {max_en_seq_len}\")\n",
    "print(f\"Max number of characters per text for outputs (French): {max_fr_seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "<b> Exercise 2.3 </b>\n",
    "- Build a vocabulary for the input text (English) and a vocabulary for the target text (French) using only <b>train dataset</b>. What is the size of each vocabulary ? \n",
    "- Build an input token-to-index dictionary to map each character (key) from the previously constructed English vocabulary to a unique index (value). \n",
    "- Build a target token-to-index dictionary to map each character (key) from the previously constructed French vocabulary to a unique index (value).\n",
    "Make sure that the character <code>\\t</code> is associated with index 0 and <code>\\n</code> is associated with index 1.     \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocabulary: 50\n",
      "Size of French vocabulary: 69\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab_en = np.unique(df_mt_train[\"english\"].map(lambda x: set(x)).explode())\n",
    "vocab_en.sort()\n",
    "vocab_fr = np.unique(df_mt_train[\"french\"].map(lambda x: set(x)).explode())\n",
    "vocab_fr.sort()\n",
    "print(\"Size of English vocabulary:\", len(vocab_en))\n",
    "print(\"Size of French vocabulary:\", len(vocab_fr))\n",
    "\n",
    "# Token to index lookup dicts for english and french\n",
    "en_token2index_dict = {char: i for i, char in enumerate(vocab_en)}\n",
    "fr_token2index_dict = {char: i for i, char in enumerate(vocab_fr)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs. Inference in Seq2Seq Models\n",
    "\n",
    "Although the architecture of a Seq2Seq model (encoder + decoder) stays the same, its **behavior during training and inference is very different** — and it’s important to understand why.\n",
    "\n",
    "##### **Training Phase** – *Using the Right Answer*\n",
    "\n",
    "During training, we use a method called **teacher forcing**. This means that at each time step, the decoder is given the **correct previous token** from the target sentence (french) — not the one the model predicted.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/teacher_forcing.png\" width=\"40%\"/>\n",
    "  <figcaption>Author: Wanshun Wong<figcaption/>\n",
    "</div>\n",
    "\n",
    "At each step, the model learns to predict the **next token** given the **true previous one**. This makes training faster and more stable because the decoder always knows the \"right context,\" even if it would have made a mistake on its own.\n",
    "\n",
    "**The model is trained to minimize the difference between its predicted output sequence and the true output sequence, using a loss function like cross-entropy.**\n",
    "\n",
    "##### **Inference Phase** – *Using Its Own Predictions*\n",
    "\n",
    "When we switch to inference (i.e., generating a translation), the model doesn’t have access to the ground-truth translation anymore. It has to **generate the output one token at a time**, feeding **its own previous prediction** back into the decoder.\n",
    "\n",
    "It starts with the **start-of-sequence token (SOS)** and continues predicting the next token based on everything it has generated so far. It stops when it outputs the **end-of-sequence token (EOS)** or hits a maximum sequence length.\n",
    "\n",
    "This process is called **autoregressive decoding**, and it's harder because:\n",
    "- There is no ground truth to guide the model\n",
    "- One small error early on can affect all the next predictions\n",
    "\n",
    "##### Summary\n",
    "\n",
    "| Phase     | What is fed to the decoder?          | Purpose                            |\n",
    "|-----------|--------------------------------------|------------------------------------|\n",
    "| Training  | The correct previous token (teacher forcing) | To help the model learn faster and more accurately |\n",
    "| Inference | The model’s own previous prediction  | To test whether the model can generate coherent sequences by itself |\n",
    "\n",
    "Understanding this distinction is key: **teacher forcing helps the model learn**, while **inference checks whether it really has**.\n",
    "\n",
    "\n",
    "---- \n",
    "\n",
    "Knowing this, let's proceed to the vectorization using one-hot encoding. \n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 2.4 </b>\n",
    "\n",
    "- Write a function `one_hot_input` that prepares:\n",
    "    - the encoder's inputs `encoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_english_sentence_length, size_english_vocabulary)` containing the one-hot vectorization of the English sentences. The sentences shorter than `max_english_sentence_length` are to be filled with spaces `\" \"` (padding). Replace out-of-vocabulary tokens by a space `\" \"`.\n",
    "    - the decoder's inputs `decoder_one_hot_inputs` as a 3D array of shape `(size_corpus, max_french_sentence_length, size_french_vocabulary)` containing the one-hot vectorization of the French sentences. The sentences shorter than `max_french_sentence_length` are to be filled with spaces `\" \"`. Replace out-of-vocabulary tokens by a space `\" \"`. \n",
    "</div>\n",
    "\n",
    "Consistently with the teacher forcing approach, notice how the function `one_hot_target` that prepares the decoder's target `decoder_one_hot_targets` is the same as `decoder_one_hot_inputs` but offset by one step: `decoder_one_hot_targets[:, j, :]` $\\leftarrow$ `decoder_one_hot_inputs[:, j + 1, :]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_input(corpus, token2index_dict, max_seq_len):\n",
    "    result = np.zeros(\n",
    "        (len(corpus), max_seq_len, len(token2index_dict)), dtype=\"float32\"\n",
    "    )\n",
    "    for i, document in enumerate(corpus):\n",
    "        for t, char in enumerate(document):\n",
    "            try:\n",
    "                result[i, t, token2index_dict[char]] = 1.0\n",
    "            except:\n",
    "                result[i, t, token2index_dict[\" \"]] = 1.0\n",
    "                continue\n",
    "        result[i, t + 1 :, token2index_dict[\" \"]] = 1.0\n",
    "    return result\n",
    "\n",
    "\n",
    "def one_hot_target(corpus, token2index_dict, max_seq_len):\n",
    "    result = np.zeros(\n",
    "        (len(corpus), max_seq_len, len(token2index_dict)), dtype=\"float32\"\n",
    "    )\n",
    "    for i, document in enumerate(corpus):\n",
    "        for t, char in enumerate(document):\n",
    "            if t > 0:\n",
    "                try:\n",
    "                    result[i, t - 1, token2index_dict[char]] = 1.0\n",
    "                except:\n",
    "                    result[i, t - 1, token2index_dict[\" \"]] = 1.0\n",
    "                    continue\n",
    "        result[i, t:, token2index_dict[\" \"]] = 1.0\n",
    "    return result\n",
    "\n",
    "\n",
    "encoder_one_hot_inputs = one_hot_input(\n",
    "    df_mt_train.english, en_token2index_dict, max_en_seq_len\n",
    ")\n",
    "decoder_one_hot_inputs = one_hot_input(\n",
    "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
    ")\n",
    "decoder_one_hot_targets = one_hot_target(\n",
    "    df_mt_train.french, fr_token2index_dict, max_fr_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training \n",
    "\n",
    "\n",
    "We will now implement the architecture shown in the figure below to train a Seq2Seq model for character-level machine translation.\n",
    "\n",
    "On the left, the encoder processes a sequence of one-hot encoded input characters (from an English sentence) using a stack of GRU cells. As the encoder reads each character, it updates its internal hidden state. Once the final character is processed, the last hidden state becomes the context vector summarizing the entire input sentence.\n",
    "\n",
    "This context vector is then passed to the decoder on the right. The decoder is also composed of GRU cells, and at each time step, it receives:\n",
    "\n",
    "* The one-hot encoded representation of the previous character (from the target sentence during training, or from its own prediction during inference)\n",
    "\n",
    "* The current hidden state (initially, the final hidden state from the encoder)\n",
    "\n",
    "The decoder's GRU outputs are passed through a Dense layer followed by a softmax activation, producing a probability distribution over the target vocabulary at each time step. This allows the model to predict the next character in the output sequence. The decoder continues this autoregressive process until it generates a special end-of-sequence (EOS) token.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figures/seq2seqmodel.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 3.1 </b>\n",
    "\n",
    "- Briefly explain how a GRU layer works: What are its inputs, outputs, and why is it useful in the Seq2Seq context? \n",
    "- Examine and complement the implementation of the `seq2seq_model` function to build the Seq2Seq model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:17:41.167435: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-25 18:17:41.167476: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-25 18:17:41.167511: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 18:17:41.174421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-25 18:17:44.217528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 512  # dim of the latent space.\n",
    "\n",
    "\n",
    "def seq2seq_model(latent_dim, input_tokenidx_dict, target_tokenidx_dict):\n",
    "    # Define an input sequence and process it.\n",
    "    x_encoder = tf.keras.Input(shape=(None, len(input_tokenidx_dict)))\n",
    "    encoder = tf.keras.layers.GRU(latent_dim, return_state=True)\n",
    "    y_encoder, h_encoder = encoder(x_encoder)\n",
    "\n",
    "    # Set up the decoder input sequence\n",
    "    x_decoder = tf.keras.Input(shape=(None, len(target_tokenidx_dict)))\n",
    "\n",
    "    # We set up the decoder to return output sequences and hidden states\n",
    "    # We don't use the return states in the training model, but we will use them in inference.\n",
    "    decoder = tf.keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "    y_decoder, _ = decoder(x_decoder, initial_state=h_encoder)\n",
    "    # Output layer, Dense + softmax activation on\n",
    "    dense_softmax = tf.keras.layers.Dense(\n",
    "        len(target_tokenidx_dict), activation=\"softmax\"\n",
    "    )\n",
    "    y_decoder = dense_softmax(y_decoder)\n",
    "\n",
    "    # Define the model that will turn [`x_encoder`, `x_decoder`] into `y_decoder`\n",
    "    model = tf.keras.Model([x_encoder, x_decoder], y_decoder)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b> Exercise 3.2 </b>\n",
    "\n",
    "- Instantiate and train the Seq2Seq model using the teacher forcing approach.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:17:49.018559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.018851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.043772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.044037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.044234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.044425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.186783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.186996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.187176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.187347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.187529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.187699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.197747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.197955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.198137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.198311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.198497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.198639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n",
      "2025-03-25 18:17:49.198970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-25 18:17:49.199112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 880 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:61:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:17:51.806411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8801\n",
      "2025-03-25 18:17:51.871894: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x753666876f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-25 18:17:51.871924: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-03-25 18:17:51.871932: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-03-25 18:17:51.876133: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-25 18:17:51.928726: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 4s 6ms/step - loss: 0.9619 - accuracy: 0.7483 - val_loss: 0.7198 - val_accuracy: 0.7907\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.8021 - val_loss: 0.6459 - val_accuracy: 0.8103\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6224 - accuracy: 0.8160 - val_loss: 0.6100 - val_accuracy: 0.8184\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.5862 - accuracy: 0.8256 - val_loss: 0.5718 - val_accuracy: 0.8283\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.5527 - accuracy: 0.8346 - val_loss: 0.5389 - val_accuracy: 0.8384\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.5211 - accuracy: 0.8438 - val_loss: 0.5133 - val_accuracy: 0.8455\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.4928 - accuracy: 0.8519 - val_loss: 0.4827 - val_accuracy: 0.8550\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.4645 - accuracy: 0.8596 - val_loss: 0.4595 - val_accuracy: 0.8618\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.4391 - accuracy: 0.8675 - val_loss: 0.4340 - val_accuracy: 0.8699\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.4168 - accuracy: 0.8740 - val_loss: 0.4191 - val_accuracy: 0.8735\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.3968 - accuracy: 0.8795 - val_loss: 0.4015 - val_accuracy: 0.8786\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3783 - accuracy: 0.8850 - val_loss: 0.3852 - val_accuracy: 0.8841\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3609 - accuracy: 0.8901 - val_loss: 0.3749 - val_accuracy: 0.8873\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3450 - accuracy: 0.8951 - val_loss: 0.3629 - val_accuracy: 0.8907\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3304 - accuracy: 0.8991 - val_loss: 0.3529 - val_accuracy: 0.8943\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3166 - accuracy: 0.9035 - val_loss: 0.3464 - val_accuracy: 0.8954\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.3061 - accuracy: 0.9066 - val_loss: 0.3377 - val_accuracy: 0.8989\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2925 - accuracy: 0.9102 - val_loss: 0.3328 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2811 - accuracy: 0.9137 - val_loss: 0.3273 - val_accuracy: 0.9021\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2701 - accuracy: 0.9168 - val_loss: 0.3243 - val_accuracy: 0.9029\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2596 - accuracy: 0.9200 - val_loss: 0.3165 - val_accuracy: 0.9050\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.2495 - accuracy: 0.9228 - val_loss: 0.3151 - val_accuracy: 0.9061\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2400 - accuracy: 0.9257 - val_loss: 0.3126 - val_accuracy: 0.9070\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.2307 - accuracy: 0.9283 - val_loss: 0.3136 - val_accuracy: 0.9078\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2219 - accuracy: 0.9310 - val_loss: 0.3105 - val_accuracy: 0.9093\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.2133 - accuracy: 0.9336 - val_loss: 0.3109 - val_accuracy: 0.9094\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2051 - accuracy: 0.9361 - val_loss: 0.3119 - val_accuracy: 0.9100\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1975 - accuracy: 0.9381 - val_loss: 0.3104 - val_accuracy: 0.9107\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1899 - accuracy: 0.9404 - val_loss: 0.3111 - val_accuracy: 0.9116\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1828 - accuracy: 0.9427 - val_loss: 0.3166 - val_accuracy: 0.9104\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1763 - accuracy: 0.9445 - val_loss: 0.3138 - val_accuracy: 0.9124\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1700 - accuracy: 0.9467 - val_loss: 0.3162 - val_accuracy: 0.9127\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1638 - accuracy: 0.9484 - val_loss: 0.3224 - val_accuracy: 0.9112\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1583 - accuracy: 0.9501 - val_loss: 0.3202 - val_accuracy: 0.9128\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1529 - accuracy: 0.9519 - val_loss: 0.3248 - val_accuracy: 0.9129\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1477 - accuracy: 0.9536 - val_loss: 0.3258 - val_accuracy: 0.9135\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1432 - accuracy: 0.9549 - val_loss: 0.3300 - val_accuracy: 0.9131\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1387 - accuracy: 0.9562 - val_loss: 0.3309 - val_accuracy: 0.9133\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1345 - accuracy: 0.9576 - val_loss: 0.3353 - val_accuracy: 0.9127\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1303 - accuracy: 0.9588 - val_loss: 0.3388 - val_accuracy: 0.9129\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1264 - accuracy: 0.9599 - val_loss: 0.3410 - val_accuracy: 0.9132\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1230 - accuracy: 0.9611 - val_loss: 0.3407 - val_accuracy: 0.9145\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1196 - accuracy: 0.9621 - val_loss: 0.3453 - val_accuracy: 0.9138\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1161 - accuracy: 0.9630 - val_loss: 0.3478 - val_accuracy: 0.9144\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1132 - accuracy: 0.9639 - val_loss: 0.3501 - val_accuracy: 0.9144\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1104 - accuracy: 0.9646 - val_loss: 0.3533 - val_accuracy: 0.9150\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1075 - accuracy: 0.9654 - val_loss: 0.3583 - val_accuracy: 0.9137\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1045 - accuracy: 0.9664 - val_loss: 0.3584 - val_accuracy: 0.9147\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1023 - accuracy: 0.9670 - val_loss: 0.3619 - val_accuracy: 0.9146\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0995 - accuracy: 0.9679 - val_loss: 0.3646 - val_accuracy: 0.9147\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0974 - accuracy: 0.9685 - val_loss: 0.3675 - val_accuracy: 0.9147\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0950 - accuracy: 0.9691 - val_loss: 0.3691 - val_accuracy: 0.9147\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0922 - accuracy: 0.9699 - val_loss: 0.3693 - val_accuracy: 0.9155\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0902 - accuracy: 0.9705 - val_loss: 0.3781 - val_accuracy: 0.9142\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0884 - accuracy: 0.9711 - val_loss: 0.3798 - val_accuracy: 0.9144\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0861 - accuracy: 0.9718 - val_loss: 0.3828 - val_accuracy: 0.9147\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0844 - accuracy: 0.9722 - val_loss: 0.3837 - val_accuracy: 0.9147\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0824 - accuracy: 0.9728 - val_loss: 0.3853 - val_accuracy: 0.9157\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0804 - accuracy: 0.9734 - val_loss: 0.3925 - val_accuracy: 0.9147\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0787 - accuracy: 0.9741 - val_loss: 0.3892 - val_accuracy: 0.9153\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0770 - accuracy: 0.9745 - val_loss: 0.3962 - val_accuracy: 0.9152\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0747 - accuracy: 0.9753 - val_loss: 0.3994 - val_accuracy: 0.9147\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0729 - accuracy: 0.9757 - val_loss: 0.4032 - val_accuracy: 0.9150\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0718 - accuracy: 0.9760 - val_loss: 0.4010 - val_accuracy: 0.9153\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0700 - accuracy: 0.9767 - val_loss: 0.4035 - val_accuracy: 0.9158\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.4066 - val_accuracy: 0.9157\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0671 - accuracy: 0.9775 - val_loss: 0.4137 - val_accuracy: 0.9146\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0660 - accuracy: 0.9778 - val_loss: 0.4147 - val_accuracy: 0.9150\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0640 - accuracy: 0.9784 - val_loss: 0.4192 - val_accuracy: 0.9146\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0630 - accuracy: 0.9788 - val_loss: 0.4205 - val_accuracy: 0.9149\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0617 - accuracy: 0.9792 - val_loss: 0.4207 - val_accuracy: 0.9152\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0604 - accuracy: 0.9797 - val_loss: 0.4266 - val_accuracy: 0.9151\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.4263 - val_accuracy: 0.9153\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0580 - accuracy: 0.9804 - val_loss: 0.4305 - val_accuracy: 0.9149\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0566 - accuracy: 0.9808 - val_loss: 0.4333 - val_accuracy: 0.9154\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0555 - accuracy: 0.9811 - val_loss: 0.4369 - val_accuracy: 0.9150\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0541 - accuracy: 0.9816 - val_loss: 0.4369 - val_accuracy: 0.9152\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0529 - accuracy: 0.9821 - val_loss: 0.4407 - val_accuracy: 0.9155\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0521 - accuracy: 0.9822 - val_loss: 0.4435 - val_accuracy: 0.9146\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0514 - accuracy: 0.9825 - val_loss: 0.4436 - val_accuracy: 0.9153\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0500 - accuracy: 0.9831 - val_loss: 0.4502 - val_accuracy: 0.9153\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0494 - accuracy: 0.9832 - val_loss: 0.4497 - val_accuracy: 0.9152\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 0.4556 - val_accuracy: 0.9145\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0479 - accuracy: 0.9837 - val_loss: 0.4532 - val_accuracy: 0.9151\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0464 - accuracy: 0.9841 - val_loss: 0.4561 - val_accuracy: 0.9155\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0456 - accuracy: 0.9844 - val_loss: 0.4606 - val_accuracy: 0.9154\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0454 - accuracy: 0.9845 - val_loss: 0.4645 - val_accuracy: 0.9148\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.4643 - val_accuracy: 0.9151\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0440 - accuracy: 0.9850 - val_loss: 0.4715 - val_accuracy: 0.9139\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0435 - accuracy: 0.9851 - val_loss: 0.4694 - val_accuracy: 0.9146\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0427 - accuracy: 0.9854 - val_loss: 0.4678 - val_accuracy: 0.9156\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0421 - accuracy: 0.9855 - val_loss: 0.4720 - val_accuracy: 0.9158\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0411 - accuracy: 0.9861 - val_loss: 0.4748 - val_accuracy: 0.9149\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 0.4743 - val_accuracy: 0.9151\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 0.4771 - val_accuracy: 0.9147\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0396 - accuracy: 0.9863 - val_loss: 0.4796 - val_accuracy: 0.9147\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0390 - accuracy: 0.9866 - val_loss: 0.4803 - val_accuracy: 0.9151\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0383 - accuracy: 0.9869 - val_loss: 0.4846 - val_accuracy: 0.9155\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: 0.4852 - val_accuracy: 0.9155\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.4861 - val_accuracy: 0.9151\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and compile it\n",
    "model = seq2seq_model(latent_dim, en_token2index_dict, fr_token2index_dict)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Set early stopping if validation accuracy does not improve after `patience` epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", patience=50, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    [encoder_one_hot_inputs, decoder_one_hot_inputs],\n",
    "    decoder_one_hot_targets,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 50)]           0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None, 69)]           0         []                            \n",
      "                                                                                                  \n",
      " gru (GRU)                   [(None, 512),                866304    ['input_1[0][0]']             \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                 [(None, None, 512),          895488    ['input_2[0][0]',             \n",
      "                              (None, 512)]                           'gru[0][1]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 69)             35397     ['gru_1[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1797189 (6.86 MB)\n",
      "Trainable params: 1797189 (6.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Inference \n",
    "\n",
    "Unlike training, the target sequence is not known during inference. The model generates the output sequence character by character, starting from an SOS character until it generates an EOS character, signifying the end of the output sequence. Since the true output tokens are not available, the model uses its own predictions as input for the next step. This process is autoregressive and continues until the model produces the EOS or reaches a maximum length. \n",
    "\n",
    "During inference, the model generates the target sequence character by character, in an autoregressive way. \n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 4.1 </b>\n",
    "\n",
    "- Examine the implementation of the `translate` function and analyse how the Seq2Seq model is used for inference.\n",
    "- Run the inference on some test sentences one by one.\n",
    "- What do you notice about errors with this autoregressive generation? Are mistakes equally distributed throughout a sequence (begining, middle and end)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Sentence 0 --------------------\n",
      "Input sentence: do it tomorrow.\n",
      "Translated sentence: arrête de faire ça !\n",
      "\n",
      "-------------------- Sentence 1 --------------------\n",
      "Input sentence: i felt ill.\n",
      "Translated sentence: je me sentais malade.\n",
      "\n",
      "-------------------- Sentence 2 --------------------\n",
      "Input sentence: it'll be easy.\n",
      "Translated sentence: ça sera facile.\n",
      "\n",
      "-------------------- Sentence 3 --------------------\n",
      "Input sentence: i got hot.\n",
      "Translated sentence: j'ai compris.\n",
      "\n",
      "-------------------- Sentence 4 --------------------\n",
      "Input sentence: who broke this?\n",
      "Translated sentence: qui l'a betion ?\n",
      "\n",
      "-------------------- Sentence 5 --------------------\n",
      "Input sentence: i'm no quitter.\n",
      "Translated sentence: je ne suis pas une sainte.\n",
      "\n",
      "-------------------- Sentence 6 --------------------\n",
      "Input sentence: he's so stupid.\n",
      "Translated sentence: il fur inutile.\n",
      "\n",
      "-------------------- Sentence 7 --------------------\n",
      "Input sentence: did tom eat?\n",
      "Translated sentence: est-ce que tom a mangé ?\n",
      "\n",
      "-------------------- Sentence 8 --------------------\n",
      "Input sentence: enough is enough.\n",
      "Translated sentence: il suffit !\n",
      "\n",
      "-------------------- Sentence 9 --------------------\n",
      "Input sentence: i see a queen.\n",
      "Translated sentence: je vois un livre.\n",
      "\n",
      "-------------------- Sentence 10 --------------------\n",
      "Input sentence: i was wrong.\n",
      "Translated sentence: j'eus tort.\n",
      "\n",
      "-------------------- Sentence 11 --------------------\n",
      "Input sentence: i hate flying.\n",
      "Translated sentence: je déteste conduire.\n",
      "\n",
      "-------------------- Sentence 12 --------------------\n",
      "Input sentence: is it your book?\n",
      "Translated sentence: est-ce que tom est paresseux ?\n",
      "\n",
      "-------------------- Sentence 13 --------------------\n",
      "Input sentence: i'm on vacation.\n",
      "Translated sentence: je ne suis pas une sainte.\n",
      "\n",
      "-------------------- Sentence 14 --------------------\n",
      "Input sentence: now i know why.\n",
      "Translated sentence: non, pas demportique.\n",
      "\n",
      "-------------------- Sentence 15 --------------------\n",
      "Input sentence: i felt naked.\n",
      "Translated sentence: je me sentais seul.\n",
      "\n",
      "-------------------- Sentence 16 --------------------\n",
      "Input sentence: call the manager.\n",
      "Translated sentence: appelez le responsable.\n",
      "\n",
      "-------------------- Sentence 17 --------------------\n",
      "Input sentence: leave a comment.\n",
      "Translated sentence: laisse un commentaire.\n",
      "\n",
      "-------------------- Sentence 18 --------------------\n",
      "Input sentence: i rescheduled.\n",
      "Translated sentence: j'ai remis.\n",
      "\n",
      "-------------------- Sentence 19 --------------------\n",
      "Input sentence: help me.\n",
      "Translated sentence: aidez-moi.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(input_seq, seq2seq_model=model):\n",
    "    # You can check seq2seq_model.summary() to recall the input and outputs\n",
    "    # of its layers\n",
    "\n",
    "    # Encoder\n",
    "    x_encoder = seq2seq_model.input[0]  # input_1\n",
    "    y_encoder, h_encoder = seq2seq_model.layers[2].output  # lstm_1\n",
    "    encoder = tf.keras.Model(inputs=x_encoder, outputs=h_encoder)\n",
    "\n",
    "    # Decoder\n",
    "    x_decoder = seq2seq_model.input[1]  # input_2\n",
    "    h_in_decoder = tf.keras.Input(shape=(latent_dim,))\n",
    "    gru_layer = seq2seq_model.layers[3]\n",
    "    y_gru, h_out_gru = gru_layer(x_decoder, initial_state=h_in_decoder)\n",
    "    dense_layer = seq2seq_model.layers[4]\n",
    "    y_decoder = dense_layer(y_gru)\n",
    "    decoder_model = tf.keras.Model([x_decoder, h_in_decoder], [y_decoder, h_out_gru])\n",
    "\n",
    "    # Reverse dictionary to recover target vocabulary from token index\n",
    "    token_idx_fr_dict = dict((i, char) for char, i in fr_token2index_dict.items())\n",
    "\n",
    "    def decode_translation(input_seq):\n",
    "        # Encode input as context vector.\n",
    "        h_encoder = encoder.predict(input_seq, verbose=0)\n",
    "\n",
    "        # Generate empty target sequence of length 1.\n",
    "        translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
    "        # Initialize the first character of target sequence with the SOS.\n",
    "        translated_seq[0, 0, fr_token2index_dict[\"\\t\"]] = 1.0\n",
    "\n",
    "        # Sampling loop for a batch (=1) of sequences\n",
    "        condition = False\n",
    "        decoded_sentence = \"\"\n",
    "        h_in_decoder = h_encoder  # init\n",
    "        while not condition:\n",
    "            y_decoder, h_out_decoder = decoder_model.predict(\n",
    "                [translated_seq, h_in_decoder], verbose=0\n",
    "            )\n",
    "\n",
    "            # Get token\n",
    "            token_index = np.argmax(y_decoder[0, -1, :])\n",
    "            char = token_idx_fr_dict[token_index]\n",
    "            decoded_sentence += char\n",
    "\n",
    "            # Stop condition: find stop character or reach max length\n",
    "            if char == \"\\n\" or len(decoded_sentence) > max_fr_seq_len:\n",
    "                condition = True\n",
    "\n",
    "            # Update the translated sequence.\n",
    "            translated_seq = np.zeros((1, 1, len(fr_token2index_dict)))\n",
    "            translated_seq[0, 0, token_index] = 1.0\n",
    "\n",
    "            # Update hidden state\n",
    "            h_in_decoder = h_out_decoder\n",
    "        return decoded_sentence\n",
    "\n",
    "    return decode_translation(input_seq)\n",
    "\n",
    "\n",
    "# test input sequences\n",
    "input_sequences = one_hot_input(df_mt_test.english, en_token2index_dict, max_en_seq_len)\n",
    "\n",
    "\n",
    "for i, seq_index in enumerate(range(20)):\n",
    "\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences[seq_index : seq_index + 1]\n",
    "    # encoder_one_hot_inputs[seq_index : seq_index + 1]\n",
    "    translated_sentence = translate(input_seq)\n",
    "\n",
    "    print(f\"-------------------- Sentence {i} --------------------\")\n",
    "    print(\"Input sentence:\", df_mt_test.english[seq_index])\n",
    "    print(\"Translated sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU score is one of the most common automatic metrics used to evaluate the quality of machine translation models. It measures how closely the model’s output matches one or more reference translations.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "<b> Exercise 4.2 </b>\n",
    "\n",
    "* Briefly explain how the BLEU score works. What does it measure, and how is it computed?\n",
    "\n",
    "* Using `nltk.translate.bleu_score`, compute the average BLEU score over at least 100 sentence pairs (predicted vs. reference). Interpret the result: what does it say about the model's performance?\n",
    "\n",
    "* Suggest some ways to improve the model. For example, you can consider data quality, preparation, vectorization or model architecture.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a38c21dc24837143ba33216130b9f8da84e53ab59c3fe234ca08dab4ce64daf"
  },
  "kernelspec": {
   "display_name": "puncc-dev-env",
   "language": "python",
   "name": "puncc-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
