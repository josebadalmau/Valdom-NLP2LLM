{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imxXAN7jDg2d"
      },
      "source": [
        "# Finetuning\n",
        "The purpose of this notebook is to learn how to finetune a transformer model for the task of machine translation. This notebook is heavily inspired by the following huggingface tutorials:\n",
        "- [Tutorial on Translation](https://huggingface.co/docs/transformers/tasks/translation)\n",
        "- [Course on Translation](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)\n",
        "\n",
        "We start by importing the numpy library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FLyqG8xBAnXk"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1GgsBElEM8W"
      },
      "source": [
        "We are going to use the *T5 transformer* model that we have already tested in the previous notebook. We are going to fine-tune the *T5 model* on the ``europarl`` dataset, a dataset containing text from the European Parliament Proceedings. You can check out more information about this dataset on its [huggingface dataset card](https://huggingface.co/datasets/Helsinki-NLP/europarl)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHav7JuXRSw7"
      },
      "source": [
        "## Data preparation\n",
        "We will first download the subset of the ``europarl`` dataset that contains english text and its counterpart french translations. In order to do so, make sure that you have the Hugging Face library ``datasets`` installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx5VrR-UHb2I"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR-XQDNiJQhr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\")\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEBDlG_dGPw2"
      },
      "source": [
        "**Questions.** Answer the following questions:\n",
        "1. What type of object is the ``raw_dataset`` object ?\n",
        "2. How many elements are there in the ``raw_dataset`` object ?\n",
        "3. What type of object is the ``raw_dataset[\"train\"]`` object ?\n",
        "4. Describe the ``raw_dataset[\"train\"]`` object.\n",
        "\n",
        "**Exercise.** Print one of the elements of ``raw_dataset[\"train\"]``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJnJyO5BSbjs"
      },
      "outputs": [],
      "source": [
        "# TODO: print one of the elements of raw_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaqbXEIbHTbd"
      },
      "source": [
        "The next step is to split the data into a proper training set and a test/validation set on which we can monitor our finetuning process.\n",
        "\n",
        "**Exercise.** Create a new ``split_dataset`` object containing a train and a test subset of the original dataset, by randomly splitting the original dataset with the following proportions: 90% training and 10% test. Print the new ``split_dataset`` object.\n",
        "\n",
        "**Hint.** ``raw_dataset[\"train\"]`` is a ``Dataset`` object, and dataset objects have a method called ``train_test_split`` which should come in handy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDXf2ysuSp2V"
      },
      "outputs": [],
      "source": [
        "# TODO: create the split_dataset object\n",
        "\n",
        "# TODO: print the split_dataset object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsmaiAAUOW5i"
      },
      "source": [
        "## Model\n",
        "As mentioned earlier, we are going to use the model *T5 tranformer* model provided by Google. Since we are going to be finetuning the model, and finetuning takes quite a long time, we will choose to use the *small* version of the *T5 model*, rather than the *base* version like last time.\n",
        "\n",
        "**Question.** Search online and compare the number of parameters of the *T5 base* and the *T5 small* models.\n",
        "\n",
        "**Exercise.** Check on the Hugging Face Hub for the name of the *T5 small* model checkpoint and store it in the variable ``model_checkpoint.``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "55x42Ob5PXWf"
      },
      "outputs": [],
      "source": [
        "# TODO: create the variable model_checkpoint with the appropriate model checkpoint string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV9DaX2K9BSd"
      },
      "source": [
        "## Tokenizer\n",
        "The next step is to tokenize our inputs. In order to do so, we will instantiate a ``tokenizer`` which Hugging Face will guess by using the ``AutoTokenizer`` class. We only need to tell the ``AutoTokenizer`` what model checkpoint we will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujXy9Rhm8pit"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeTj6k-oRH7V"
      },
      "source": [
        "**Exercise.** Let's find out more about the tokenizer we will be using. Check online for the different attributes of the ``Tokenizer`` class, and write code in order to answer the following questions:\n",
        "1. What is the name of the tokenizer being used?\n",
        "2. What is the size of the vocabulary?\n",
        "3. What is the maximum model input length?\n",
        "4. What special tokens does the tokenizer use? What are their IDs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjjb6hehR1Tt"
      },
      "outputs": [],
      "source": [
        "# TODO: print the necessary information about the automatically load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D75CZtMISWRf"
      },
      "source": [
        "We next try out our tokenizer in a few input sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kIRluqytMPc7"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    \"Hello, My name is John\",\n",
        "    \"I love ice cream\",\n",
        "    \"The grey cat slept on the chair.\",\n",
        "    \"When is Rodrigo coming home?\",\n",
        "    \"The quick brown dog jumps over the lazy dog.\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Urs6Ysd8LCO"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "print(inputs[\"input_ids\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abqOWXOANKXK"
      },
      "source": [
        "**Questions.** Think about the following questions (do not hesitate to discuss them with your classmates and with the teacher) ignoring the ``attention_mask`` part for now:\n",
        "1. What type of structure is the output of ``inputs``?\n",
        "2. What is the size of the tensor ``inputs[input_ids]``? What does this size represent?\n",
        "4. What is the last non-zero integer of each row in ``inputs[input_ids]``? Why?\n",
        "5. What does the 0 element represent in the tensor ``inputs[input_ids]``?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_xB1tpd-CD8"
      },
      "source": [
        "### Preprocessing function\n",
        "Before tokenizing our inputs, we will preprocess them for easier use with the ``T5 model``.  In order to do so, we will define a preprocessing function called ``preprocess_function`` that does the following:\n",
        "1. Prepend the phrase \"Translate from English to French:\" to the source English test. Remember that otherwise T5 will try and translate to German!\n",
        "2. Set the target (French) in the ``text_target`` parameter to ensure the tokenizer processes the target text correctly. Otherwise the tokenizer assumes the language is English.\n",
        "3. Truncate the sequences to be no longer then the maximum length set by the ``max_length`` parameter. We will set it to 128.\n",
        "4. Tokenize the inputs by taking into account all of the above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o0jyZnGc9UpM"
      },
      "outputs": [],
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"fr\"\n",
        "prefix = \"Translate from English to French: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
        "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfxUHAMBWeoH"
      },
      "source": [
        "We next use use our preprocessing function to tokenize the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMprZxAx-pQW"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = split_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5TeQJwkWy0F"
      },
      "source": [
        "Lastly, we use the ``DataCollatorForSeq2Seq`` class in order to *dynamically pad* the sentences to the longest length in a batch, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oZS4akqm-thy"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTqyc4f4IfOH"
      },
      "source": [
        "## Metrics\n",
        "During the finetuning phase, the model parameters will be optimized by gradient descent on the Cross-Entropy Loss, as seen in class. However, we can monitor if our model is indeed learning/overfitting by using more interpretable metrics on the validation dataset. In this case, we will use the SacreBLEU metric. Make sure you have the ``evaluate`` and ``sacrebleu`` libraries installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cy6OVJCQfP_"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1hv_wQf_KHz"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric  = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0a6uVPBXyow"
      },
      "source": [
        "The following two functions will process the predictions of the models as well as the ground truth labels and compute the SacreBLEU score associated to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kxyfjRd3_8vr"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # replace the -100 token_id by 0 in the labels\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
        "    ]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJvGKwCZAsxc"
      },
      "source": [
        "## Fine-tuning\n",
        "We now proceed to the actual finetuning phase. In order to fine-tune our model we will need to:\n",
        "1. Instantiate our model (recall that so-far we have just stored the name of the model checkpoint in ``model_checkpoint`` variable) with the help of the ``AutoModelForSeq2SeqLM`` class.\n",
        "2. Set all the hyperparameters and other relevant arguments for the training phase with the help of the ``Seq2SeqTrainingArguments`` class.\n",
        "3. Train the model with the help of the ``Seq2SeqTrainer`` class.\n",
        "\n",
        "**Exercise.** Instantiate the model by using the ``AutoModelForSeq2SeqLM`` class of the ``transformers`` library and the previously defined ``model_checkpoint``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASK60EClAk9H"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# TODO: instantialte the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5PrwyDgaMOI"
      },
      "source": [
        "The model is already pretrained, which means that it is already able to translate from English to French.\n",
        "\n",
        "**Exercise.** Test the model on the following sentences\n",
        "\n",
        "\n",
        "> The farmers took the cows up to the mountains.\n",
        "\n",
        "> He finally kicked the bucket after years of saying he wasn't ready to.\n",
        "\n",
        "> When the engineer spoke to the manager about the delay, he admitted it was his fault.\n",
        "\n",
        "> You might want to reconsider how directly you addressed the committee.\n",
        "\n",
        "> It's not so much that the plan failed as that it was never really tested.\n",
        "\n",
        "> By the time she realized what had happened, the opportunity had already slipped away.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTOnNqRYhVL"
      },
      "outputs": [],
      "source": [
        "# TODO: test the model in the given sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUi6ZWXZaiKg"
      },
      "source": [
        "**Exercise.** Complete the training argumets with the following hyper-parameters:\n",
        "- A learning rate of 0.00002\n",
        "- A batch size of 32 for the training phase\n",
        "- A batch size of 64 for the evaluation phase\n",
        "- 1 epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ytc4Cyg5BSmU"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"europarlament_en_fr_translator\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    # TODO: specify the learning rate\n",
        "    # TODO: specify the trainig batch size\n",
        "    # TODO: specify the evaluation batch size\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    # TODO: specify the number of epochs\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALS2-nUAbYxB"
      },
      "source": [
        "The last step is to train the model. Since our training and validation datasets are quite large, the finetuning phase will take very long, even if we train it for 1 epoch only. Therefore, we will truncate the training and validation datasets to speed-up the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8BCxLQxJd92"
      },
      "outputs": [],
      "source": [
        "n_train = 100_000\n",
        "n_val = 10_000\n",
        "\n",
        "train_subset = tokenized_dataset[\"train\"].select(range(n_train))\n",
        "val_subset = tokenized_dataset[\"test\"].select(range(n_val))\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=val_subset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Evaluation at Epoch 0:\", metrics)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X83K430-b0JJ"
      },
      "source": [
        "## Inference\n",
        "We can now use our finetuned model for translating English sentences into French.\n",
        "\n",
        "**Exercise.** Translate the same sentence as above with the finetuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMnskVxcDqw"
      },
      "outputs": [],
      "source": [
        "# TODO: translate the same sentences again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLjlk9zRcc_A"
      },
      "source": [
        "We can change the generation type so as to randomize the model's output, and not always provide the same translation for a given input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sVlzX-vdC_w7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd74d312-a248-4d68-a157-91c52cc6d6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les agriculteurs détournent la vache vers les montagnes.\n",
            "Les producteurs permettent aux vaches d'accéder dans la montagne.\n",
            "Les agriculteurs transportent les vaches au-dessus des montagnes.\n",
            "Les fermiers transportent les vaches dans les montagnes.\n",
            "Les agriculteurs ramènent les vaches en montagne.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "for _ in range(5):\n",
        "    output = model.generate(**input_text, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
        "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KplyTZtRC-lV"
      },
      "source": [
        "**Discussion.** Discuss with the classmates and with the teacher.\n",
        "- What steps of the above notebook are clear ?\n",
        "- What steps of the above notebook are unclear ?\n",
        "- What is the BLEU metric measuring ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCa0jYoeDV7j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}