{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxnNxp2ukEzf"
      },
      "source": [
        "# Tiny Transformer Summarizer from Scratch\n",
        "The objective of this notebook is to train a LLM from scratch on a small dataset for text summarization. The model will be trained directly on the *text summarization* task. As last week, we will continue working on:\n",
        "1. Data preparation\n",
        "2. Building the LLM architecture\n",
        "3. Training an LLM\n",
        "\n",
        "However, there are two main differences with respect to the *transformer classifier* that we trained last week:\n",
        "- *Encoder-Decoder architecture.* In order to be able to perform the text summarization task, we will train a transformer having an *encoder-decoder* type of architecture (or Seq2Seq architecture). The encoder part is quite similar to that of an encoder-only model, we will focus on the decoder part.\n",
        "- *Maksing.* We will also focus on the masking technique. Masking is important to make sure that the decoder does not look into future tokens or the current token during training. We will also use masking in order to ignore the padding tokens that we will add to complete the different batches.\n",
        "\n",
        "We start with the usual library imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyvKSV7tldOH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac31Z7WU0a9v"
      },
      "source": [
        "## Encoder-decoder architecture\n",
        "Since we are training a *transformer model* for the task of text summarization, an *encoder-decoder* transformer is necessary. Our encoder-decoder model will follow the classical transformer architecture, with the only difference that we will only be stacking a single encoder block and a single decoder block.\n",
        "\n",
        "We will start by implementing all the necessary sub-modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qjxg1HHQ0GL"
      },
      "source": [
        "### Multi-Head Attention\n",
        "The *multi-head* attention layer takes the input and processes it in chunks of equal length through eacc of its different heads, by using the attention mechanism, i.e. computing the queries, keys and values, and the attention scores and weights from them.\n",
        "\n",
        "We will modify the `MultiHeadAttention` class that we coded last week in order to:\n",
        "- Allow for queries, keys and values coming from different input (important for the *decoder cross-attention* mechanism).\n",
        "- Allow for *masking:* the `forward` method of the class will take an additional optional `mask` argument that we will apply to the attention scores before the softmax computation.\n",
        "\n",
        "**Question.** Since we are masking attention scores *before* taking the softmax, what value should we replace the masked entries with?\n",
        "\n",
        "**Exercise.** Choose an appropriate value for the masking process in the `attention`method of the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpQVacoIon-7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads  # Compute the per-head hidden dimension\n",
        "\n",
        "        self.W_query = nn.Linear(hidden_dim, hidden_dim) # queries weight matrix\n",
        "        self.W_key = nn.Linear(hidden_dim, hidden_dim) # keys weight matrix\n",
        "        self.W_value = nn.Linear(hidden_dim, hidden_dim) # values weight matrix\n",
        "\n",
        "        self.W_out = nn.Linear(hidden_dim, hidden_dim)  # output weight matrix\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(queries, keys, values, mask):\n",
        "        head_dim = queries.shape[-1]\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = torch.einsum(\"bijk, bikl -> bijl\", queries, keys.transpose(2, 3))\n",
        "        attn_scores = attn_scores / math.sqrt(head_dim)\n",
        "\n",
        "        # TODO: Add appropriate value for masking\n",
        "        if mask is not None:\n",
        "            attn_scores.masked_fill_(mask == 0, # TODO: Add appropriate value)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Attention output\n",
        "        attn_output = torch.einsum(\"bijk, bikl -> bijl\", attn_weights, values)\n",
        "\n",
        "        # Return attention outputs (and attention weights for visualization purposes)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        batch_size, _, hidden_dim = q.shape\n",
        "        assert hidden_dim == self.hidden_dim, f\"hidden_dim must be {self.hidden_dim}\"\n",
        "\n",
        "        queries = self.W_query(q) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        keys = self.W_key(k)  # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        values = self.W_value(v) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        queries = queries.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_tokens, head_dim)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention)\n",
        "        attn_output = self.attention(queries, keys, values, mask)\n",
        "\n",
        "        # Transpose back: (batch_size, num_heads, num_tokens, head_dim) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Concatenate heads: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_tokens, hiddend_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # Compute output\n",
        "        output = self.W_out(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYjb6MijolfL"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7vjOACIv5Lrs"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads  # Compute the per-head hidden dimension\n",
        "\n",
        "        self.W_query = nn.Linear(hidden_dim, hidden_dim) # queries weight matrix\n",
        "        self.W_key = nn.Linear(hidden_dim, hidden_dim) # keys weight matrix\n",
        "        self.W_value = nn.Linear(hidden_dim, hidden_dim) # values weight matrix\n",
        "\n",
        "        self.W_out = nn.Linear(hidden_dim, hidden_dim)  # output weight matrix\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(queries, keys, values, mask):\n",
        "        head_dim = queries.shape[-1]\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = torch.einsum(\"bijk, bikl -> bijl\", queries, keys.transpose(2, 3))\n",
        "        attn_scores = attn_scores / math.sqrt(head_dim)\n",
        "\n",
        "        # Apply masking\n",
        "        if mask is not None:\n",
        "            attn_scores.masked_fill_(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Attention output\n",
        "        attn_output = torch.einsum(\"bijk, bikl -> bijl\", attn_weights, values)\n",
        "\n",
        "        # Return attention outputs (and attention weights for visualization purposes)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        batch_size, _, hidden_dim = q.shape\n",
        "        assert hidden_dim == self.hidden_dim, f\"hidden_dim must be {self.hidden_dim}\"\n",
        "\n",
        "        queries = self.W_query(q) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        keys = self.W_key(k)  # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        values = self.W_value(v) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        queries = queries.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_tokens, head_dim)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention)\n",
        "        attn_output = self.attention(queries, keys, values, mask)\n",
        "\n",
        "        # Transpose back: (batch_size, num_heads, num_tokens, head_dim) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Concatenate heads: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_tokens, hiddend_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # Compute output\n",
        "        output = self.W_out(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5UkUGSQ281"
      },
      "source": [
        "### Layer Norm\n",
        "The `LayerNorm` module is the same as lask week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3tRQ-Eip0mZ"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5 # small value to avoid division by zero\n",
        "        self.scale = nn.Parameter(torch.ones(hidden_dim)) # scale parameter (learnable)\n",
        "        self.shift = nn.Parameter(torch.zeros(hidden_dim)) # shift parameter (learnable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        y = self.scale * norm_x + self.shift\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz8bLo4OQ5RT"
      },
      "source": [
        "### Feed-Forward Network\n",
        "The *feed forward* module is the same as last week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uycJaSs72Fx"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnp_1xu3Q8Mb"
      },
      "source": [
        "### Transformer Encoder Block\n",
        "We will keep the same architecture for the `TransformerEncoderBlock`as we did last week. However, we have changed the `MultiHeadAttention` class which now takes its inputs differently.\n",
        "\n",
        "**Exercise.** Implement the `TransformerEncoderBlock` module below by completing the `TODO` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4JgROHEp67t"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(hidden_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "    self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "    self.feed_forward = FeedForward(hidden_dim)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "      attn_output = # TODO: apply the attention module to the correct inputs\n",
        "      x = x + attn_output\n",
        "      x = self.norm1(x)\n",
        "      ff_output = self.feed_forward(x)\n",
        "      x = x + ff_output\n",
        "      x = self.norm2(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L80ih1Dv922b"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "izQHSy-o9y2e"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(hidden_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "    self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "    self.feed_forward = FeedForward(hidden_dim)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "      attn_output = self.attention(x, x, x, mask)\n",
        "      x = x + attn_output\n",
        "      x = self.norm1(x)\n",
        "      ff_output = self.feed_forward(x)\n",
        "      x = x + ff_output\n",
        "      x = self.norm2(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Y8LJesybLY"
      },
      "source": [
        "### Transformer Decoder Block\n",
        "We will now build the *Transformer decoder block*, which is composed of:\n",
        "1. A *Multi-head attention* sub-module, which will perform the self-attention on the decoder input, and will apply both a padding mask and a causal mask.\n",
        "2. A *LayerNorm* normalization.\n",
        "3. A *Multi-head attention* sub-module, which will perform the cross-attention between the decoder hidden state and the encoder hidden state, and will apply the encoder padding mask.\n",
        "4. A *LayerNorm* normalization\n",
        "5. A *Feed Forward* sub-module.\n",
        "6. A *LayerNorm* normalization\n",
        "\n",
        "Moreover, three skip-connections are present in the *Transformer Block*:\n",
        "- *(A)* A skip connection that adds the original input to the output of the operation *1* above.\n",
        "- *(B)* A skip connection that adds the output of the operation *2* to the output of the operation *3* above.\n",
        "- *(C)* A skip connection that adds the output of the operation *4* to the output of the operation *5* above\n",
        "\n",
        "**Exercise.** Implement the `TransformerDecoderBlock` module below by completing the `TODO` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C0qcxCNyeVQ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = # TODO: initialize the self attention module with the appropriate dimensions\n",
        "        self.norm1 = # TODO: initialize the first Layer Norm with the appropriate dimensions\n",
        "        self.cross_attention = # TODO: initialize the self attention module with the appropriate dimensions\n",
        "        self.norm2 = # TODO: initialize the second Layer Norm with the appropriate dimensions\n",
        "        self.feed_forward = # TODO: initialize the Feed Forward module with the appropriate dimensions\n",
        "        self.norm3 = # TODO: initialize the third Layer Norm with the appropriate dimensions\n",
        "\n",
        "    def forward(self, x, enc_hidden_state, src_mask, tgt_mask):\n",
        "        tgt_mask = # TODO: add causal mask to target mask by using the add_causal_mask function below\n",
        "        # TODO: implement the rest of the forward pass of the TransformerDecoderBLock\n",
        "        return x\n",
        "\n",
        "    def add_causal_mask(mask):\n",
        "        # assuming attn scores of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "        # assuming mask of shape (batch_size, 1, 1, seq_len)\n",
        "        causal_mask = torch.triu(torch.ones(1, 1, mask.size(-1), mask.size(-1)), diagonal=1).type(torch.int)  # shape (1, 1, seq_len, seq_len)\n",
        "        causal_mask = causal_mask == 0\n",
        "        causal_mask.to(device)\n",
        "        return mask & causal_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDW42owjvDSU"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kqj4MyN3vB1N"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.cross_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.feed_forward = FeedForward(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x, enc_hidden_state, src_mask, tgt_mask):\n",
        "        tgt_mask = add_causal_mask(tgt_mask) # add causal mask\n",
        "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = x + attn_output\n",
        "        x = self.norm1(x)\n",
        "        attn_output = self.cross_attention(x, enc_hidden_state, enc_hidden_state, src_mask)\n",
        "        x = x + attn_output\n",
        "        x = self.norm2(x)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + ff_output\n",
        "        x = self.norm3(x)\n",
        "        return x\n",
        "\n",
        "    def add_causal_mask(mask):\n",
        "        # assuming attn scores of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "        # assuming mask of shape (batch_size, 1, 1, seq_len)\n",
        "        causal_mask = torch.triu(torch.ones(1, 1, mask.size(-1), mask.size(-1)), diagonal=1).type(torch.int)  # shape (1, 1, seq_len, seq_len)\n",
        "        causal_mask = causal_mask == 0\n",
        "        causal_mask.to(device)\n",
        "        return mask & causal_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci9cZ_FzREGD"
      },
      "source": [
        "### Embedding\n",
        "The `Embedding` module is the same as for the encoder-only model of last week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1dfuX3hme-g"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "    self.position_encoding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, seq_length = x.shape\n",
        "    token_embeddings = self.embedding(x)\n",
        "    pos_encodings = self.position_encoding(torch.arange(seq_length, device=x.device))\n",
        "    return token_embeddings + pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9X5IMMJQ_Q6"
      },
      "source": [
        "### Transformer Encoder-Decoder Model for Text Summarization\n",
        "We have implemented all necessary sub-modules ane we are now ready to implement the *end-to-end* architecture of the *transformer encoder-decoder* model. To that end, we will make use of:\n",
        "- The `Embedding` sub-modules (one for the encoder, one for the decoder).\n",
        "- The `TransformerEncoderBlock` sub-module\n",
        "- The `TransformerDecoderBlock` sub-module\n",
        "- A `nn.Linear` layer to act as a classification head.\n",
        "\n",
        "**Note.** The output of the `TransformerDecoderBlock` sub-module is a tensor of shape $(b, s, d_h)$ where $b$ represents the batch size, $s$ the sequence length, and $d_h$ the hidden dimension of the model. In order to perform the classification task, we need to compute one logit per token and per sequence in the batch.\n",
        "\n",
        "**Exercise.** Implement the `TransformerSummarizer` model by completing the `TODO` tags below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5GodkC2o86a"
      },
      "outputs": [],
      "source": [
        "class TransformerSummarizer(nn.Module):\n",
        "    def __init__(self,\n",
        "               vocab_size,\n",
        "               encoder_max_length,\n",
        "               decoder_max_length,\n",
        "               hidden_dim,\n",
        "               num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_embedding = # TODO: initialize the encoder embedding with the appropriate parameters\n",
        "        self.tgt_embedding = # TODO: initialize the decoder embedding with the appropriate parameters\n",
        "        self.encoder = # TODO: initialize the encoder with the appropriate parameters\n",
        "        self.decoder = # TODO: initialize the decoder with the appropriate parameters\n",
        "        self.summarizer_head = # TODO: initialize the classifier head with the appropriate parameters\n",
        "\n",
        "\n",
        "    def encode(self, x_src, src_mask):\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2) # (b, seq_len) -> (b, 1, 1, seq_len)\n",
        "        x_src = # TODO: compute the embedding of x_src\n",
        "        x_src = # TODO: compute the encoder hidden state of x\n",
        "        return x_src\n",
        "\n",
        "\n",
        "    def decode(self, x_tgt, encoder_hidden_state, src_mask, tgt_mask):\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2) # (b, seq_len) -> (b, 1, 1, seq_len)\n",
        "        tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2) # (b, seq_len) -> (b, 1, 1, seq_len)\n",
        "        x_tgt = # TODO: compute the embedding of x_tgt\n",
        "        x_tgt = # TODO: compute the decoder hidden state of x_tgt\n",
        "        return x_tgt\n",
        "\n",
        "\n",
        "    def forward(self, x_src, x_tgt, src_mask, tgt_mask):\n",
        "        # TODO: implement the forward pass and return the logit values\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FI9jQ8dHLOe"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AobzrsaMxC59"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class TransformerSummarizer(nn.Module):\n",
        "    def __init__(self,\n",
        "               vocab_size,\n",
        "               encoder_max_length,\n",
        "               decoder_max_length,\n",
        "               hidden_dim,\n",
        "               num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_embedding = Embedding(vocab_size, encoder_max_length, hidden_dim)\n",
        "        self.tgt_embedding = Embedding(vocab_size, decoder_max_length, hidden_dim)\n",
        "        self.encoder = TransformerEncoderBlock(hidden_dim, num_heads)\n",
        "        self.decoder = TransformerDecoderBlock(hidden_dim, num_heads)\n",
        "        self.summarizer_head = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "\n",
        "    def encode(self, x_src, src_mask):\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "        x_src = self.src_embedding(x_src)\n",
        "        x_src = self.encoder(x_src, src_mask)\n",
        "        return x_src\n",
        "\n",
        "\n",
        "    def decode(self, x_tgt, encoder_hidden_state, src_mask, tgt_mask):\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2)\n",
        "        x_tgt = self.tgt_embedding(x_tgt)\n",
        "        x_tgt = self.decoder(x_tgt, encoder_hidden_state, src_mask, tgt_mask)\n",
        "        return x_tgt\n",
        "\n",
        "\n",
        "    def forward(self, x_src, x_tgt, src_mask, tgt_mask):\n",
        "        encoder_hidden_state = self.encode(x_src, src_mask)\n",
        "        x_tgt = self.decode(x_tgt, encoder_hidden_state, src_mask, tgt_mask)\n",
        "        logits = self.summarizer_head(x_tgt)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrNr7bGo2DNw"
      },
      "source": [
        "## Data Preparation\n",
        "We will train the `TransformerSummarizer` model on a text summarization task. The data set we will be using is the following [dataset](https://huggingface.co/datasets/knkarthick/dialogsum). It consists of pairs of text of type `document` and `summary`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru7fxIQ12GfJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "raw_dataset = load_dataset(\"knkarthick/dialogsum\")\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEBDlG_dGPw2"
      },
      "source": [
        "**Questions.** Describe the datasets above.\n",
        "\n",
        "**Exercise.** Print one of the elements of ``raw_dataset[\"train\"]``, both the document and its summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjgYHlNTWQKc"
      },
      "outputs": [],
      "source": [
        "# TODO: print one of the elements of raw_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ykAagWTzGq0"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VbXV_fj02byC"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(\"Document:\")\n",
        "print(raw_dataset['train'][0]['dialogue'])\n",
        "print(\"\")\n",
        "print(\"Summary:\")\n",
        "print(raw_dataset['train'][0]['summary'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8h6rkiczbyH"
      },
      "source": [
        "The above `raw_dataset` is quite large, and training the transformer on it will take too much time. In order to be able to carry our experiments in a reasonable time, we will reduce the size of the dataset.\n",
        "\n",
        "**Exercise.** Create a `tiny_dataset` by keeping the first 500 training exmamples, the first 100 validation examples, and the first 100 test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isx03-rl0oEf"
      },
      "outputs": [],
      "source": [
        "# TODO: Create the tiny_dataset object\n",
        "print(tiny_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nceq9-V0BPb"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "PuxdZm9Nz-gN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "tiny_dataset = {\n",
        "    \"train\": raw_dataset[\"train\"].select(range(500)),  # Keep first 5,000 examples\n",
        "    \"validation\": raw_dataset[\"validation\"].select(range(100)),  # Keep first 1_000\n",
        "    \"test\": raw_dataset[\"test\"].select(range(100))  # Keep first 1_000\n",
        "}\n",
        "\n",
        "tiny_dataset = DatasetDict(tiny_dataset)\n",
        "tiny_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdBHC_KyXlPj"
      },
      "source": [
        "### Pre-trained Tokenizer\n",
        "In order to convert the textual data in the above datasets into a format that can be used as input for our models, we first *tokenize* the text using a pre-trained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqUZDpAy2hyI"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcgDdKIZLkM-"
      },
      "source": [
        "**Exercise.** Let's find out more about the tokenizer we will be using. Write code in order to answer the following questions:\n",
        "1. What is the name of the tokenizer being used?\n",
        "2. What is the size of the vocabulary?\n",
        "3. What is the maximum model input length?\n",
        "4. What special tokens does the tokenizer use? What are their IDs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMeEIYU5L5EV"
      },
      "outputs": [],
      "source": [
        "# TODO: print the necessary information about the automatically load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n1W6NF4SFSG"
      },
      "source": [
        "**Solution.** Click below to check the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uZ60haBBJ3-d"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(f\"Name of the tokenizer: {tokenizer.__class__}\")\n",
        "print(f\"Size of the vocabulary: {tokenizer.vocab_size}\")\n",
        "print(f\"Maximum model input length: {tokenizer.model_max_length}\")\n",
        "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
        "\n",
        "for key, value in tokenizer.special_tokens_map.items():\n",
        "    print(f\"{key}: {value}; token_id: {tokenizer.convert_tokens_to_ids(value)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCccTXC3MGB"
      },
      "source": [
        "### Pre-processing the data\n",
        "We next build a pre-processing function in order to:\n",
        "1. Tokenize the document and summaries.\n",
        "2. Generate the appropriate inputs and outputs.\n",
        "\n",
        "Note that our model will be trained using:\n",
        "- The document as the input to the encoder.\n",
        "- The summary as the input to the decoder.\n",
        "- The summary with the tokens shifter right as the target output.\n",
        "\n",
        "In order to produce the decoder input-output pairs, we will suppress the *end-of-sequence* token from the decoder input, and the *beginning-of-sequence* token from the target output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYTCghcs2P-n"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "ENCODER_MAX_LEN = 1024\n",
        "DECODER_MAX_LEN = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRgWsWY_VXr"
      },
      "outputs": [],
      "source": [
        "def preprocessing_function(example):\n",
        "    model_inputs = {}\n",
        "    encoder_inputs = tokenizer(example[\"dialogue\"], max_length=ENCODER_MAX_LEN, padding='max_length', truncation=True)\n",
        "    model_inputs[\"encoder_input_ids\"] = encoder_inputs[\"input_ids\"]\n",
        "    model_inputs[\"encoder_mask\"] = encoder_inputs[\"attention_mask\"]\n",
        "    labels = tokenizer(example[\"summary\"], max_length=DECODER_MAX_LEN + 1, padding='max_length', truncation=True)\n",
        "    eos_index = labels[\"input_ids\"].index(tokenizer.eos_token_id)\n",
        "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"][:eos_index]+labels[\"input_ids\"][eos_index+1:]\n",
        "    model_inputs[\"decoder_mask\"] = labels[\"attention_mask\"][:eos_index] + labels[\"attention_mask\"][eos_index+1:]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"][1:]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEQsYPTDPcNF"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization to the entire dataset\n",
        "tokenized_dataset = tiny_dataset.map(preprocessing_function, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jabW7xQZAUlD"
      },
      "outputs": [],
      "source": [
        "# Remove the original text fields to save memory\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"dialogue\", \"summary\", \"id\"])\n",
        "\n",
        "# Print dataset structure\n",
        "print(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98g_OGBmYlT7"
      },
      "source": [
        "### Data Loaders\n",
        "Finally, we create three separate pytorch data loaders in order to easily iterate through them during the training and evaluation phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHeRUmo_3hvS"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset.set_format(type=\"torch\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"validation\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aRl_d2Ib_c9"
      },
      "source": [
        "## Training and Evaluation\n",
        "Now that we have both our model architecture defined and our data prepared, we can proceed to the last phase of the lab project: training and evaluating the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfcU1lT6Iqg"
      },
      "source": [
        "### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDody-lZ5cEB"
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
        "print(f\"ENCODER_MAX_LEN: {ENCODER_MAX_LEN}\")\n",
        "print(f\"DECODER_MAX_LEN: {DECODER_MAX_LEN}\")\n",
        "print(f\"HIDDEN_DIM: {HIDDEN_DIM}\")\n",
        "print(f\"NUM_HEADS: {NUM_HEADS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGTtAxL5vz4"
      },
      "outputs": [],
      "source": [
        "summarizer = TransformerSummarizer(VOCAB_SIZE, ENCODER_MAX_LEN, DECODER_MAX_LEN, HIDDEN_DIM, NUM_HEADS)\n",
        "print(summarizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07SxarFxdYC8"
      },
      "source": [
        "### Train Model\n",
        "We can now proceed to the model training phase. In order to do so, we will define two functions:\n",
        "- A `train_epoch` function that will train the model by iterating through the given data loader once.\n",
        "- An `evaluate` function that will evaluate the model on the given data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91w1FRgh56eg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qth2nl9C6PjI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoder_input_ids = batch[\"encoder_input_ids\"]\n",
        "        encoder_mask = batch[\"encoder_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_mask = batch[\"decoder_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(encoder_input_ids, decoder_input_ids, encoder_mask, decoder_mask)\n",
        "        outputs = outputs.view(-1, VOCAB_SIZE)\n",
        "        labels = labels.view(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tx3H5oW6Rg4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
        "\n",
        "            encoder_input_ids = batch[\"encoder_input_ids\"]\n",
        "            encoder_mask = batch[\"encoder_mask\"]\n",
        "            decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "            decoder_mask = batch[\"decoder_mask\"]\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            outputs = model(encoder_input_ids, decoder_input_ids, encoder_mask, decoder_mask)\n",
        "            outputs = outputs.view(-1, VOCAB_SIZE)\n",
        "            labels = labels.view(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoyUPvBu6USI"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "optimizer = torch.optim.Adam(summarizer.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train_epoch(summarizer, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(summarizer, val_loader, criterion)\n",
        "\n",
        "    epoch_time = time.time()\n",
        "\n",
        "    print(\"\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_time}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd9x3tEErDmd"
      },
      "source": [
        "### Evaluation\n",
        "Finally, we evaluate the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzucyYNNiAgQ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = evaluate(summarizer, test_loader, criterion)\n",
        "print(\"\")\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYDqE4J2JetB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
